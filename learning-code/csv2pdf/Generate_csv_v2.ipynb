{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa42fb68",
   "metadata": {},
   "source": [
    "# ğŸ“˜ general csv\n",
    "é€‚é…å¤æ‚ AT å‘½ä»¤æ‰‹å†Œï¼ˆåµŒå¥—è¡¨ã€åˆå¹¶å•å…ƒæ ¼ã€å¤šå‚æ•°è¡¨ï¼‰ã€‚è¾“å…¥ `AT_Commands.docx`ï¼Œè¾“å‡º HTMLï¼ˆå¯é€‰ PDFï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b30d8f",
   "metadata": {},
   "source": [
    "## Step 0 â€” å®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3387efce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ä¾èµ–å®‰è£…å®Œæˆ\n"
     ]
    }
   ],
   "source": [
    "!pip install -q python-docx pandas pyyaml jinja2 sphinx sphinx_rtd_theme lxml\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d6be7",
   "metadata": {},
   "source": [
    "## Step 0.5 â€” é…ç½®ä¸é€šç”¨å·¥å…·ï¼ˆè·¯å¾„ã€æ—¥å¿—ã€ç›®å½•æŸ¥çœ‹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "099f5a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é…ç½®å°±ç»ªï¼›å¯ç”¨ print_tree('docs') æŸ¥çœ‹ Sphinx ç›®å½•ç»“æ„\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, traceback, datetime, subprocess, sys, shutil\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from docx import Document\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.oxml.table import CT_Tbl\n",
    "\n",
    "IN_WORD = \"SimpleAT.docx\"\n",
    "DATA_DIR = \"data\"\n",
    "CSV_OUT  = os.path.join(DATA_DIR, \"demoAT.csv\")\n",
    "YAML_OUT = os.path.join(DATA_DIR, \"at_all_commands.yaml\")\n",
    "RST_DIR  = os.path.join(DATA_DIR, \"rst_output\")\n",
    "DOCS_DIR = \"docs\"\n",
    "LOG_PATH = \"parse_log.txt\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RST_DIR, exist_ok=True)\n",
    "\n",
    "def log(msg: str):\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{datetime.datetime.now().isoformat(timespec='seconds')}] {msg}\\n\")\n",
    "\n",
    "open(LOG_PATH, \"w\", encoding=\"utf-8\").write(\"\")\n",
    "\n",
    "def print_tree(root=\"docs\"):\n",
    "    if not os.path.exists(root):\n",
    "        print(f\"(ä¸å­˜åœ¨) {root}\")\n",
    "        return\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        level = dirpath.replace(root, \"\").count(os.sep)\n",
    "        indent = \"  \" * level\n",
    "        print(f\"{indent}{os.path.basename(dirpath)}/\")\n",
    "        subindent = \"  \" * (level + 1)\n",
    "        for f in filenames:\n",
    "            print(f\"{subindent}{f}\")\n",
    "print(\"âœ… é…ç½®å°±ç»ªï¼›å¯ç”¨ print_tree('docs') æŸ¥çœ‹ Sphinx ç›®å½•ç»“æ„\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1718dfb",
   "metadata": {},
   "source": [
    "## Step 1 â€” Word â†’ CSVï¼ˆå¤æ‚è§£æï¼šåµŒå¥—è¡¨ã€å¤šè¡¨åˆå¹¶ã€valmapï¼‰\n",
    "- è¯†åˆ«å‘½ä»¤æ ‡é¢˜ï¼›åˆå¹¶åç»­è¯´æ˜æ®µï¼›\n",
    "- è¿ç»­å‚æ•°è¡¨è‡ªåŠ¨åˆå¹¶ï¼›\n",
    "- ä¼˜å…ˆè§£æåµŒå¥—è¡¨ä¸º `valmap`ï¼Œå¦åˆ™å›é€€æ–‡æœ¬è§£æï¼›\n",
    "- æ—¥å¿—å†™å…¥ `parse_log.txt`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CMD_LINE_PAT = re.compile(r'^\\s*(AT[\\+\\w\\-]+(?:\\?[=\\w<>,\\s\\-\\+\\.\\:]*?)?)\\s*(?::|ï¼š)?\\s*(.*)$', re.I)\n",
    "PARAM_HEADING_PAT = re.compile(r'^\\s*å‚æ•°(è¯´æ˜|è¡¨|ä¿¡æ¯)?\\s*[:ï¼š]?\\s*$', re.I)\n",
    "\n",
    "def is_cmd_heading(text: str) -> bool: return bool(CMD_LINE_PAT.match(text or \"\"))\n",
    "def is_param_heading(text: str) -> bool: return bool(PARAM_HEADING_PAT.match(text or \"\"))\n",
    "\n",
    "def iter_ordered_blocks(doc):\n",
    "    body = doc._element.body\n",
    "    tbl_idx = 0\n",
    "    for child in body.iterchildren():\n",
    "        if isinstance(child, CT_P):\n",
    "            text = \"\".join([t.text for t in child.xpath('.//w:t') if t.text]).strip()\n",
    "            yield (\"p\", text)\n",
    "        elif isinstance(child, CT_Tbl):\n",
    "            table_obj = doc.tables[tbl_idx]\n",
    "            tbl_idx += 1\n",
    "            yield (\"tbl\", table_obj)\n",
    "\n",
    "def cell_plain_text(cell):\n",
    "    parts = [p.text.strip() for p in cell.paragraphs if p.text and p.text.strip()]\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "def find_nested_tbls_in_cell(cell):\n",
    "    xml_str = cell._tc.xml\n",
    "    root = etree.fromstring(xml_str.encode(\"utf-8\"))\n",
    "    ns = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "    return root.findall(\".//w:tbl\", ns), ns\n",
    "\n",
    "def tbl_rows_as_text(tbl, ns):\n",
    "    rows = []\n",
    "    for r in tbl.findall(\".//w:tr\", ns):\n",
    "        cells = r.findall(\".//w:tc\", ns)\n",
    "        row = [\"\".join(tn.text for tn in c.iterfind(\".//w:t\", ns) if tn.text).strip() for c in cells]\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def looks_like_header(row):\n",
    "    hdr = \" \".join(row[:3])\n",
    "    return any(k in hdr for k in [\"å‚æ•°\",\"åç§°\",\"Name\",\"æè¿°\",\"è¯´æ˜\",\"å«ä¹‰\",\"å–å€¼\",\"å€¼\",\"value\",\"meaning\",\"å¤‡æ³¨\",\"èŒƒå›´\"])\n",
    "\n",
    "def nested_table_to_valmap(rows):\n",
    "    if not rows: return {}\n",
    "    start = 1 if looks_like_header(rows[0]) else 0\n",
    "    kv = {}\n",
    "    for r in rows[start:]:\n",
    "        if not r: continue\n",
    "        key = (r[0] or \"\").strip()\n",
    "        val = \" | \".join([c for c in r[1:] if c and c.strip()]) if len(r) > 1 else \"\"\n",
    "        if key: kv[key] = val\n",
    "    return kv\n",
    "\n",
    "def cell_valmap_from_nested_table(cell):\n",
    "    tbls, ns = find_nested_tbls_in_cell(cell); mapping = {}\n",
    "    for t in tbls:\n",
    "        rows = tbl_rows_as_text(t, ns); mapping.update(nested_table_to_valmap(rows))\n",
    "    return mapping\n",
    "\n",
    "def parse_enum_map_fuzzy(text):\n",
    "    if not text: return {}\n",
    "    segs = re.split(r\"[,\\uFF0C;\\uFF1B\\n]+\", text.strip())\n",
    "    m = {}\n",
    "    for s in segs:\n",
    "        s = s.strip()\n",
    "        if not s: continue\n",
    "        if \":\" in s or \"ï¼š\" in s:\n",
    "            k, v = re.split(r\"[:ï¼š]\", s, 1); k, v = k.strip(), v.strip()\n",
    "        else:\n",
    "            m2 = re.match(r\"^(\\S+)\\s*(?:->|â†’|=>|-|â€”|\\s)\\s*(.+)$\", s)\n",
    "            if m2: k, v = m2.group(1).strip(), m2.group(2).strip()\n",
    "            else:\n",
    "                m3 = re.match(r\"^([A-Za-z0-9\\+\\-\\.]+)\\s+(.+)$\", s)\n",
    "                if m3: k, v = m3.group(1).strip(), m3.group(2).strip()\n",
    "                else: continue\n",
    "        if k: m[k] = v\n",
    "    return m\n",
    "\n",
    "def extract_word_to_csv(docx_path, csv_out):\n",
    "    if not os.path.exists(docx_path):\n",
    "        raise FileNotFoundError(f\"æœªæ‰¾åˆ° Word æ–‡ä»¶: {docx_path}\")\n",
    "    log(f\"Start parsing: {docx_path}\")\n",
    "    doc = Document(docx_path)\n",
    "    seq = list(iter_ordered_blocks(doc))\n",
    "\n",
    "    results = []; i = 0; cmd_order = 0\n",
    "    while i < len(seq):\n",
    "        typ, obj = seq[i]\n",
    "        if typ == \"p\":\n",
    "            m = CMD_LINE_PAT.match(obj)\n",
    "            if m:\n",
    "                cmd_order += 1\n",
    "                current_cmd = m.group(1).strip()\n",
    "                current_title = (m.group(2) or \"\").strip()\n",
    "                log(f\"CMD[{cmd_order}] {current_cmd} â€” {current_title}\")\n",
    "\n",
    "                desc_lines = []; j = i + 1\n",
    "                while j < len(seq):\n",
    "                    t2, o2 = seq[j]\n",
    "                    if t2 == \"p\":\n",
    "                        if is_cmd_heading(o2) or is_param_heading(o2): break\n",
    "                        if o2: desc_lines.append(o2)\n",
    "                    elif t2 == \"tbl\": break\n",
    "                    j += 1\n",
    "                merged_desc = \"\\n\".join(desc_lines).strip()\n",
    "\n",
    "                params_all = []; table_count = 0; k = j\n",
    "                while k < len(seq):\n",
    "                    t3, o3 = seq[k]\n",
    "                    if t3 == \"p\" and is_cmd_heading(o3): break\n",
    "                    if t3 == \"p\" and is_param_heading(o3):\n",
    "                        k += 1\n",
    "                        while k < len(seq) and seq[k][0] == \"tbl\":\n",
    "                            table = seq[k][1]; table_count += 1\n",
    "                            for r in table.rows:\n",
    "                                cols = r.cells\n",
    "                                if not any(c.text.strip() for c in cols): continue\n",
    "                                try:\n",
    "                                    name = cell_plain_text(cols[0]) if len(cols) > 0 else \"\"\n",
    "                                    desc = cell_plain_text(cols[1]) if len(cols) > 1 else \"\"\n",
    "                                    valmap = {}\n",
    "                                    if len(cols) > 2:\n",
    "                                        valmap = cell_valmap_from_nested_table(cols[2]) or parse_enum_map_fuzzy(cell_plain_text(cols[2]))\n",
    "                                    if not valmap and len(cols) > 1:\n",
    "                                        valmap = cell_valmap_from_nested_table(cols[1]) or parse_enum_map_fuzzy(desc)\n",
    "                                    if name in (\"å‚æ•°\",\"å‚æ•°å\",\"Name\") and any(x in desc for x in [\"æè¿°\",\"è¯´æ˜\",\"Description\",\"Meaning\"]): \n",
    "                                        continue\n",
    "                                    params_all.append({\"name\": name, \"desc\": desc, \"valmap\": valmap})\n",
    "                                except Exception as e:\n",
    "                                    log(f\"ROW-ERROR in {current_cmd}: {e}\")\n",
    "                                    log(traceback.format_exc())\n",
    "                            k += 1\n",
    "                        continue\n",
    "                    k += 1\n",
    "\n",
    "                if params_all or merged_desc:\n",
    "                    results.append({\n",
    "                        \"ç« èŠ‚\": str(cmd_order), \"å‘½ä»¤\": current_cmd, \"å‘½ä»¤æ ‡é¢˜\": current_title, \n",
    "                        \"å‘½ä»¤ç±»å‹\": \"æ‰§è¡Œ\",  # å¯è‡ªåŠ¨åˆ¤æ–­æˆ–åç»­å†å¤„ç†\n",
    "                        \"å‘½ä»¤æ ¼å¼\": f\"{current_cmd}=<param>\" if \"<\" in current_cmd else current_cmd,\n",
    "                        \"å“åº”\": \"<CR><LF>OK<CR><LF>\",  # æˆ–è‡ªå®šä¹‰é€»è¾‘è§£æå“åº”\n",
    "                        \"ç¤ºä¾‹å‘½ä»¤\": f\"{current_cmd}\\n\\nOK\",  # æˆ–ä»æ–‡æ¡£ä¸­å®é™…æå–\n",
    "                        \"åŠŸèƒ½æè¿°\": merged_desc or current_title,\n",
    "                        \"å¤‡æ³¨\": f\"è¯´æ˜ç¤ºä¾‹{cmd_order}\",\n",
    "                        \"è¡¨æ•°é‡\": table_count,\n",
    "                        \"é¡ºåº\": cmd_order,\n",
    "                        \"å‚æ•°JSON\": json.dumps(params_all, ensure_ascii=False)\n",
    "                    })\n",
    "\n",
    "                    log(f\"CMD[{cmd_order}] tables={table_count} params={len(params_all)}\")\n",
    "                i = k; continue\n",
    "        i += 1\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(csv_out, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… æå– {len(df)} æ¡å‘½ä»¤ â†’ {csv_out}\")\n",
    "    print(f\"ğŸ“ è§£ææ—¥å¿—ï¼š{LOG_PATH}\")\n",
    "    return df\n",
    "\n",
    "df_csv = extract_word_to_csv(IN_WORD, CSV_OUT)\n",
    "df_csv.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
