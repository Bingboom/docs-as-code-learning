{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“˜ word_full_pipeline_v6.2 â€” Word â†’ CSV â†’ YAML â†’ RST â†’ Sphinx(HTML/PDF)\n",
    "é€‚é…å¤æ‚ AT å‘½ä»¤æ‰‹å†Œï¼ˆåµŒå¥—è¡¨ã€åˆå¹¶å•å…ƒæ ¼ã€å¤šå‚æ•°è¡¨ï¼‰ã€‚è¾“å…¥ `AT_Commands.docx`ï¼Œè¾“å‡º HTMLï¼ˆå¯é€‰ PDFï¼‰ã€‚\n",
    "\n",
    "**v6.2 å…³é”®å¢å¼º**\n",
    "- åˆå§‹åŒ– Sphinx å‰**è‡ªåŠ¨æ¸…ç†**æ—§é¡¹ç›®ï¼›\n",
    "- ä½¿ç”¨ `sphinx-build` æ„å»ºå¹¶**è‡ªåŠ¨é™çº§ docutils** å¤±è´¥å›é€€ï¼›\n",
    "- å»¶ç»­ v6 çš„**åµŒå¥—è¡¨ä¼˜å…ˆ / æ–‡æœ¬å›é€€**ï¼ˆç”Ÿæˆ `valmap`ï¼‰ï¼›\n",
    "- ç”Ÿæˆè§£ææ—¥å¿— `parse_log.txt`ï¼›\n",
    "- ä¸€é”® `run_all(clean=True)`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 â€” å®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -q python-docx pandas pyyaml jinja2 sphinx sphinx_rtd_theme lxml\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆ\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.5 â€” é…ç½®ä¸é€šç”¨å·¥å…·ï¼ˆè·¯å¾„ã€æ—¥å¿—ã€ç›®å½•æŸ¥çœ‹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os, re, json, traceback, datetime, subprocess, sys, shutil\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "from docx import Document\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.oxml.table import CT_Tbl\n",
    "\n",
    "IN_WORD = \"AT_Commands.docx\"\n",
    "DATA_DIR = \"data\"\n",
    "CSV_OUT  = os.path.join(DATA_DIR, \"at_extracted_commands.csv\")\n",
    "YAML_OUT = os.path.join(DATA_DIR, \"at_all_commands.yaml\")\n",
    "RST_DIR  = os.path.join(DATA_DIR, \"rst_output\")\n",
    "DOCS_DIR = \"docs\"\n",
    "LOG_PATH = \"parse_log.txt\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(RST_DIR, exist_ok=True)\n",
    "\n",
    "def log(msg: str):\n",
    "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{datetime.datetime.now().isoformat(timespec='seconds')}] {msg}\\n\")\n",
    "\n",
    "open(LOG_PATH, \"w\", encoding=\"utf-8\").write(\"\")\n",
    "\n",
    "def print_tree(root=\"docs\"):\n",
    "    if not os.path.exists(root):\n",
    "        print(f\"(ä¸å­˜åœ¨) {root}\")\n",
    "        return\n",
    "    for dirpath, dirnames, filenames in os.walk(root):\n",
    "        level = dirpath.replace(root, \"\").count(os.sep)\n",
    "        indent = \"  \" * level\n",
    "        print(f\"{indent}{os.path.basename(dirpath)}/\")\n",
    "        subindent = \"  \" * (level + 1)\n",
    "        for f in filenames:\n",
    "            print(f\"{subindent}{f}\")\n",
    "print(\"âœ… é…ç½®å°±ç»ªï¼›å¯ç”¨ print_tree('docs') æŸ¥çœ‹ Sphinx ç›®å½•ç»“æ„\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 â€” Word â†’ CSVï¼ˆå¤æ‚è§£æï¼šåµŒå¥—è¡¨ã€å¤šè¡¨åˆå¹¶ã€valmapï¼‰\n",
    "- è¯†åˆ«å‘½ä»¤æ ‡é¢˜ï¼›åˆå¹¶åç»­è¯´æ˜æ®µï¼›\n",
    "- è¿ç»­å‚æ•°è¡¨è‡ªåŠ¨åˆå¹¶ï¼›\n",
    "- ä¼˜å…ˆè§£æåµŒå¥—è¡¨ä¸º `valmap`ï¼Œå¦åˆ™å›é€€æ–‡æœ¬è§£æï¼›\n",
    "- æ—¥å¿—å†™å…¥ `parse_log.txt`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "CMD_LINE_PAT = re.compile(r'^\\s*(AT[\\+\\w\\-]+(?:\\?[=\\w<>,\\s\\-\\+\\.\\:]*?)?)\\s*(?::|ï¼š)?\\s*(.*)$', re.I)\n",
    "PARAM_HEADING_PAT = re.compile(r'^\\s*å‚æ•°(è¯´æ˜|è¡¨|ä¿¡æ¯)?\\s*[:ï¼š]?\\s*$', re.I)\n",
    "\n",
    "def is_cmd_heading(text: str) -> bool: return bool(CMD_LINE_PAT.match(text or \"\"))\n",
    "def is_param_heading(text: str) -> bool: return bool(PARAM_HEADING_PAT.match(text or \"\"))\n",
    "\n",
    "def iter_ordered_blocks(doc):\n",
    "    body = doc._element.body\n",
    "    tbl_idx = 0\n",
    "    for child in body.iterchildren():\n",
    "        if isinstance(child, CT_P):\n",
    "            text = \"\".join([t.text for t in child.xpath('.//w:t') if t.text]).strip()\n",
    "            yield (\"p\", text)\n",
    "        elif isinstance(child, CT_Tbl):\n",
    "            table_obj = doc.tables[tbl_idx]\n",
    "            tbl_idx += 1\n",
    "            yield (\"tbl\", table_obj)\n",
    "\n",
    "def cell_plain_text(cell):\n",
    "    parts = [p.text.strip() for p in cell.paragraphs if p.text and p.text.strip()]\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "def find_nested_tbls_in_cell(cell):\n",
    "    xml_str = cell._tc.xml\n",
    "    root = etree.fromstring(xml_str.encode(\"utf-8\"))\n",
    "    ns = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "    return root.findall(\".//w:tbl\", ns), ns\n",
    "\n",
    "def tbl_rows_as_text(tbl, ns):\n",
    "    rows = []\n",
    "    for r in tbl.findall(\".//w:tr\", ns):\n",
    "        cells = r.findall(\".//w:tc\", ns)\n",
    "        row = [\"\".join(tn.text for tn in c.iterfind(\".//w:t\", ns) if tn.text).strip() for c in cells]\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def looks_like_header(row):\n",
    "    hdr = \" \".join(row[:3])\n",
    "    return any(k in hdr for k in [\"å‚æ•°\",\"åç§°\",\"Name\",\"æè¿°\",\"è¯´æ˜\",\"å«ä¹‰\",\"å–å€¼\",\"å€¼\",\"value\",\"meaning\",\"å¤‡æ³¨\",\"èŒƒå›´\"])\n",
    "\n",
    "def nested_table_to_valmap(rows):\n",
    "    if not rows: return {}\n",
    "    start = 1 if looks_like_header(rows[0]) else 0\n",
    "    kv = {}\n",
    "    for r in rows[start:]:\n",
    "        if not r: continue\n",
    "        key = (r[0] or \"\").strip()\n",
    "        val = \" | \".join([c for c in r[1:] if c and c.strip()]) if len(r) > 1 else \"\"\n",
    "        if key: kv[key] = val\n",
    "    return kv\n",
    "\n",
    "def cell_valmap_from_nested_table(cell):\n",
    "    tbls, ns = find_nested_tbls_in_cell(cell); mapping = {}\n",
    "    for t in tbls:\n",
    "        rows = tbl_rows_as_text(t, ns); mapping.update(nested_table_to_valmap(rows))\n",
    "    return mapping\n",
    "\n",
    "def parse_enum_map_fuzzy(text):\n",
    "    if not text: return {}\n",
    "    segs = re.split(r\"[,\\uFF0C;\\uFF1B\\n]+\", text.strip())\n",
    "    m = {}\n",
    "    for s in segs:\n",
    "        s = s.strip()\n",
    "        if not s: continue\n",
    "        if \":\" in s or \"ï¼š\" in s:\n",
    "            k, v = re.split(r\"[:ï¼š]\", s, 1); k, v = k.strip(), v.strip()\n",
    "        else:\n",
    "            m2 = re.match(r\"^(\\S+)\\s*(?:->|â†’|=>|-|â€”|\\s)\\s*(.+)$\", s)\n",
    "            if m2: k, v = m2.group(1).strip(), m2.group(2).strip()\n",
    "            else:\n",
    "                m3 = re.match(r\"^([A-Za-z0-9\\+\\-\\.]+)\\s+(.+)$\", s)\n",
    "                if m3: k, v = m3.group(1).strip(), m3.group(2).strip()\n",
    "                else: continue\n",
    "        if k: m[k] = v\n",
    "    return m\n",
    "\n",
    "def extract_word_to_csv(docx_path, csv_out):\n",
    "    if not os.path.exists(docx_path):\n",
    "        raise FileNotFoundError(f\"æœªæ‰¾åˆ° Word æ–‡ä»¶: {docx_path}\")\n",
    "    log(f\"Start parsing: {docx_path}\")\n",
    "    doc = Document(docx_path)\n",
    "    seq = list(iter_ordered_blocks(doc))\n",
    "\n",
    "    results = []; i = 0; cmd_order = 0\n",
    "    while i < len(seq):\n",
    "        typ, obj = seq[i]\n",
    "        if typ == \"p\":\n",
    "            m = CMD_LINE_PAT.match(obj)\n",
    "            if m:\n",
    "                cmd_order += 1\n",
    "                current_cmd = m.group(1).strip()\n",
    "                current_title = (m.group(2) or \"\").strip()\n",
    "                log(f\"CMD[{cmd_order}] {current_cmd} â€” {current_title}\")\n",
    "\n",
    "                desc_lines = []; j = i + 1\n",
    "                while j < len(seq):\n",
    "                    t2, o2 = seq[j]\n",
    "                    if t2 == \"p\":\n",
    "                        if is_cmd_heading(o2) or is_param_heading(o2): break\n",
    "                        if o2: desc_lines.append(o2)\n",
    "                    elif t2 == \"tbl\": break\n",
    "                    j += 1\n",
    "                merged_desc = \"\\n\".join(desc_lines).strip()\n",
    "\n",
    "                params_all = []; table_count = 0; k = j\n",
    "                while k < len(seq):\n",
    "                    t3, o3 = seq[k]\n",
    "                    if t3 == \"p\" and is_cmd_heading(o3): break\n",
    "                    if t3 == \"p\" and is_param_heading(o3):\n",
    "                        k += 1\n",
    "                        while k < len(seq) and seq[k][0] == \"tbl\":\n",
    "                            table = seq[k][1]; table_count += 1\n",
    "                            for r in table.rows:\n",
    "                                cols = r.cells\n",
    "                                if not any(c.text.strip() for c in cols): continue\n",
    "                                try:\n",
    "                                    name = cell_plain_text(cols[0]) if len(cols) > 0 else \"\"\n",
    "                                    desc = cell_plain_text(cols[1]) if len(cols) > 1 else \"\"\n",
    "                                    valmap = {}\n",
    "                                    if len(cols) > 2:\n",
    "                                        valmap = cell_valmap_from_nested_table(cols[2]) or parse_enum_map_fuzzy(cell_plain_text(cols[2]))\n",
    "                                    if not valmap and len(cols) > 1:\n",
    "                                        valmap = cell_valmap_from_nested_table(cols[1]) or parse_enum_map_fuzzy(desc)\n",
    "                                    if name in (\"å‚æ•°\",\"å‚æ•°å\",\"Name\") and any(x in desc for x in [\"æè¿°\",\"è¯´æ˜\",\"Description\",\"Meaning\"]): \n",
    "                                        continue\n",
    "                                    params_all.append({\"name\": name, \"desc\": desc, \"valmap\": valmap})\n",
    "                                except Exception as e:\n",
    "                                    log(f\"ROW-ERROR in {current_cmd}: {e}\")\n",
    "                                    log(traceback.format_exc())\n",
    "                            k += 1\n",
    "                        continue\n",
    "                    k += 1\n",
    "\n",
    "                if params_all or merged_desc:\n",
    "                    results.append({\n",
    "                        \"å‘½ä»¤\": current_cmd, \"å‘½ä»¤æ ‡é¢˜\": current_title, \"å‘½ä»¤ç±»å‹\": \"æ‰§è¡Œ;æŸ¥è¯¢\",\n",
    "                        \"å‘½ä»¤æ ¼å¼\": current_cmd, \"ç¤ºä¾‹å‘½ä»¤\": current_cmd, \"ç¤ºä¾‹å“åº”\": \"\",\n",
    "                        \"åŠŸèƒ½æè¿°\": merged_desc or current_title, \"å¤‡æ³¨\": \"\",\n",
    "                        \"è¡¨æ•°é‡\": table_count, \"é¡ºåº\": cmd_order,\n",
    "                        \"å‚æ•°JSON\": json.dumps(params_all, ensure_ascii=False)\n",
    "                    })\n",
    "                    log(f\"CMD[{cmd_order}] tables={table_count} params={len(params_all)}\")\n",
    "                i = k; continue\n",
    "        i += 1\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(csv_out, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"âœ… æå– {len(df)} æ¡å‘½ä»¤ â†’ {csv_out}\")\n",
    "    print(f\"ğŸ“ è§£ææ—¥å¿—ï¼š{LOG_PATH}\")\n",
    "    return df\n",
    "\n",
    "df_csv = extract_word_to_csv(IN_WORD, CSV_OUT)\n",
    "df_csv.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 â€” CSV â†’ YAMLï¼ˆä¿ç•™ valmapï¼Œå¢åŠ  metaï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import yaml\n",
    "def csv_to_yaml(csv_path, yaml_path):\n",
    "    df = pd.read_csv(csv_path, dtype=str).fillna(\"\")\n",
    "    objs = []\n",
    "    for _, r in df.iterrows():\n",
    "        params = json.loads(r[\"å‚æ•°JSON\"]) if r[\"å‚æ•°JSON\"] else []\n",
    "        objs.append({\n",
    "            \"command\": r[\"å‘½ä»¤\"],\n",
    "            \"title\": r[\"å‘½ä»¤æ ‡é¢˜\"],\n",
    "            \"type\": [t.strip() for t in r[\"å‘½ä»¤ç±»å‹\"].split(\";\") if t.strip()],\n",
    "            \"formats\": [f.strip() for f in r[\"å‘½ä»¤æ ¼å¼\"].split(\"|\") if f.strip()] or [r[\"å‘½ä»¤æ ¼å¼\"]],\n",
    "            \"parameters\": params,\n",
    "            \"examples\": [],\n",
    "            \"description\": r.get(\"åŠŸèƒ½æè¿°\",\"\"),\n",
    "            \"notes\": r.get(\"å¤‡æ³¨\",\"\"),\n",
    "            \"meta\": {\"order\": int(r.get(\"é¡ºåº\",\"0\") or 0), \"tables\": int(r.get(\"è¡¨æ•°é‡\",\"0\") or 0)}\n",
    "        })\n",
    "    objs.sort(key=lambda x: x[\"meta\"][\"order\"])\n",
    "    with open(yaml_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        yaml.safe_dump({\"commands\": objs}, f, allow_unicode=True, sort_keys=False)\n",
    "    print(f\"âœ… å·²ç”Ÿæˆ YAML â†’ {yaml_path}\")\n",
    "csv_to_yaml(CSV_OUT, YAML_OUT)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 â€” YAML â†’ RSTï¼ˆvalmap æ¨¡æ¿æ¸²æŸ“ + åˆ†ç»„ç´¢å¼•ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from jinja2 import Template\n",
    "from collections import defaultdict\n",
    "\n",
    "PAGE_TMPL = Template('''\n",
    "{{ cmd.command }}\n",
    "{{ '=' * cmd.command|length }}\n",
    "\n",
    "**Title**: {{ cmd.title }}\n",
    "**Types**: {{ cmd.type|join(', ') }}\n",
    "\n",
    "Formats::\n",
    "{%- for f in cmd.formats %}\n",
    "   {{ f }}\n",
    "{%- endfor %}\n",
    "\n",
    "Parameters\n",
    "----------\n",
    ".. list-table::\n",
    "   :header-rows: 1\n",
    "   :widths: 18 34 48\n",
    "\n",
    "   * - Name\n",
    "     - Description\n",
    "     - Values\n",
    "{%- for p in cmd.parameters %}\n",
    "   * - {{ p.name }}\n",
    "     - {{ p.desc or 'â€”' }}\n",
    "     - {%- if p.valmap %}\n",
    "       .. list-table::\n",
    "          :header-rows: 1\n",
    "          :widths: 20 40\n",
    "\n",
    "          * - Key\n",
    "            - Value\n",
    "{%- for k, v in p.valmap.items() %}\n",
    "          * - {{ k }}\n",
    "            - {{ v }}\n",
    "{%- endfor %}\n",
    "       {%- else %} N/A {%- endif %}\n",
    "{%- endfor %}\n",
    "\n",
    "**Description**: {{ cmd.description or '' }}\n",
    "''')\n",
    "\n",
    "def group_key(cmd_str):\n",
    "    m = re.match(r'^AT\\+([A-Z]+)', (cmd_str or \"\").upper())\n",
    "    if not m: return \"AT-OTHER\"\n",
    "    token = m.group(1)\n",
    "    return f\"AT-{token[:2]}\" if len(token) >= 2 else \"AT-OTHER\"\n",
    "\n",
    "def yaml_to_rst(yaml_path, rst_dir):\n",
    "    with open(yaml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    cmds = data.get(\"commands\", [])\n",
    "\n",
    "    groups = defaultdict(list)\n",
    "    for cmd in cmds:\n",
    "        rst_text = PAGE_TMPL.render(cmd=cmd)\n",
    "        fname = f\"{cmd['command']}.rst\"\n",
    "        with open(os.path.join(rst_dir, fname), \"w\", encoding=\"utf-8\") as fo:\n",
    "            fo.write(rst_text)\n",
    "        groups[group_key(cmd[\"command\"])].append(cmd[\"command\"])\n",
    "\n",
    "    index_lines = [\"AT Manual\", \"=========\", \"\", \".. toctree::\", \"   :maxdepth: 1\", \"\"]\n",
    "    for g in sorted(groups.keys()):\n",
    "        grp_name = f\"index_{g}.rst\"\n",
    "        index_lines.append(f\"   {grp_name[:-4]}\")\n",
    "        glines = [g, \"=\" * len(g), \"\", \".. toctree::\", \"   :maxdepth: 1\", \"\"]\n",
    "        for c in groups[g]:\n",
    "            glines.append(f\"   {c}\")\n",
    "        with open(os.path.join(rst_dir, grp_name), \"w\", encoding=\"utf-8\") as fo:\n",
    "            fo.write(\"\\n\".join(glines))\n",
    "\n",
    "    with open(os.path.join(rst_dir, \"index.rst\"), \"w\", encoding=\"utf-8\") as fo:\n",
    "        fo.write(\"\\n\".join(index_lines))\n",
    "\n",
    "    print(f\"âœ… RST å·²ç”Ÿæˆåˆ° {rst_dir}ï¼ˆå«åˆ†ç»„ç´¢å¼•ï¼‰\")\n",
    "\n",
    "yaml_to_rst(YAML_OUT, RST_DIR)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 â€” æ¸…ç†å¹¶åˆå§‹åŒ– Sphinxï¼ˆé˜²æ­¢æ—§æ„å»ºæ±¡æŸ“ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "if os.path.exists(DOCS_DIR):\n",
    "    print(\"âš ï¸ æ£€æµ‹åˆ°æ—§ docs/ï¼Œæ­£åœ¨æ¸…ç†...\")\n",
    "    shutil.rmtree(DOCS_DIR)\n",
    "    print(\"âœ… å·²åˆ é™¤æ—§ docs/\")\n",
    "\n",
    "!sphinx-quickstart {DOCS_DIR} --sep --project \"AT Command Manual\" --author \"Doc Team\" --release \"1.0\" -q\n",
    "\n",
    "conf_py = os.path.join(DOCS_DIR, \"source\", \"conf.py\")\n",
    "with open(conf_py, \"a\", encoding=\"utf-8\") as f:\n",
    "    f.write('\\nhtml_theme = \"sphinx_rtd_theme\"\\n')\n",
    "\n",
    "shutil.copytree(RST_DIR, os.path.join(DOCS_DIR, \"source\"), dirs_exist_ok=True)\n",
    "print(\"âœ… Sphinx åˆå§‹åŒ–å®Œæˆå¹¶å¤åˆ¶ RST\")\n",
    "print_tree(\"docs\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 â€” æ„å»º HTMLï¼ˆå¤±è´¥è‡ªåŠ¨å›é€€ docutils ç‰ˆæœ¬ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "SRC_DIR = os.path.join(DOCS_DIR, \"source\")\n",
    "BUILD_DIR = os.path.join(DOCS_DIR, \"build\", \"html\")\n",
    "os.makedirs(BUILD_DIR, exist_ok=True)\n",
    "\n",
    "def build_html_with_fallback():\n",
    "    print(\"ğŸ“¦ å¼€å§‹æ„å»º HTML ...\")\n",
    "    cmd = [sys.executable, \"-m\", \"sphinx\", \"-b\", \"html\", SRC_DIR, BUILD_DIR]\n",
    "    p = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    print(p.stdout); print(p.stderr)\n",
    "    if p.returncode == 0 and os.path.exists(os.path.join(BUILD_DIR, \"index.html\")):\n",
    "        print(\"âœ… HTML æ„å»ºæˆåŠŸ â†’ docs/build/html/index.html\")\n",
    "        return True\n",
    "\n",
    "    print(\"âŒ åˆæ¬¡æ„å»ºå¤±è´¥ï¼Œå°è¯•å›é€€ docutils å¹¶é‡è¯• ...\")\n",
    "    _ = subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"--quiet\", \"docutils<0.21\"])\n",
    "    p2 = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    print(p2.stdout); print(p2.stderr)\n",
    "    if p2.returncode == 0 and os.path.exists(os.path.join(BUILD_DIR, \"index.html\")):\n",
    "        print(\"âœ… å›é€€åæ„å»ºæˆåŠŸ â†’ docs/build/html/index.html\")\n",
    "        return True\n",
    "\n",
    "    print(\"âŒ æ„å»ºå¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šé¢çš„æ—¥å¿—è¾“å‡ºã€‚\")\n",
    "    return False\n",
    "\n",
    "build_html_with_fallback()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸŸ¢ Step 6 â€” ä¸€é”®æ‰§è¡Œ `run_all(clean=True)`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def run_all(clean=True):\n",
    "    _ = extract_word_to_csv(IN_WORD, CSV_OUT)\n",
    "    csv_to_yaml(CSV_OUT, YAML_OUT)\n",
    "    yaml_to_rst(YAML_OUT, RST_DIR)\n",
    "    if clean and os.path.exists(DOCS_DIR):\n",
    "        print(\"âš ï¸ run_all: æ¸…ç†æ—§ docs/ ...\")\n",
    "        shutil.rmtree(DOCS_DIR)\n",
    "    get_ipython().system('sphinx-quickstart docs --sep --project \"AT Command Manual\" --author \"Doc Team\" --release \"1.0\" -q')\n",
    "    with open(os.path.join(DOCS_DIR, \"source\", \"conf.py\"), \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write('\\nhtml_theme = \"sphinx_rtd_theme\"\\n')\n",
    "    shutil.copytree(RST_DIR, os.path.join(DOCS_DIR, \"source\"), dirs_exist_ok=True)\n",
    "    build_html_with_fallback()\n",
    "    print(\"\\nâœ… å…¨æµç¨‹å®Œæˆã€‚HTML æŸ¥çœ‹ï¼šdocs/build/html/index.html\")\n",
    "    print(\"ğŸ“ è§£ææ—¥å¿—ï¼šparse_log.txt\")\n",
    "\n",
    "print(\"å‡†å¤‡å°±ç»ªã€‚æŒ‰é¡ºåºè¿è¡Œå„ Stepï¼Œæˆ–ç›´æ¥ run_all(clean=True)ã€‚\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
