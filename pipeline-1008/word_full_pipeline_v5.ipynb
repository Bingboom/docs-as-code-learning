{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82659a24",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ“˜ Word â†’ CSV â†’ YAML â†’ RST â†’ Sphinx å…¨è‡ªåŠ¨æµæ°´çº¿ v5\n",
    "**ä¸»è¦æ”¹è¿›ï¼š**\n",
    "- æ”¯æŒ Word åµŒå¥—è¡¨æ ¼è§£æï¼Œç”Ÿæˆ `valmap`\n",
    "- CSV / YAML ä¿ç•™å®Œæ•´å‚æ•°æ˜ å°„ç»“æ„\n",
    "- RST æ¸²æŸ“ä½¿ç”¨ `p.valmap` æ¨¡æ¿\n",
    "- å…¼å®¹å¤šè¡Œã€å†’å·ã€ç©ºæ ¼ç­‰å¤æ‚å–å€¼è¡¨è¾¾\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe648e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 0 â€” å®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰\n",
    "!pip install -q python-docx pandas pyyaml jinja2 sphinx sphinx_rtd_theme lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb10fb60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æå– 3 æ¡å‘½ä»¤ â†’ data/extracted_commands.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>å‘½ä»¤</th>\n",
       "      <th>å‘½ä»¤æ ‡é¢˜</th>\n",
       "      <th>å‘½ä»¤ç±»å‹</th>\n",
       "      <th>å‘½ä»¤æ ¼å¼</th>\n",
       "      <th>ç¤ºä¾‹å‘½ä»¤</th>\n",
       "      <th>ç¤ºä¾‹å“åº”</th>\n",
       "      <th>åŠŸèƒ½æè¿°</th>\n",
       "      <th>å¤‡æ³¨</th>\n",
       "      <th>å‚æ•°JSON</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATI</td>\n",
       "      <td>è·å–æ¨¡ç»„å‚å•†ä¿¡æ¯</td>\n",
       "      <td>æ‰§è¡Œ;æŸ¥è¯¢</td>\n",
       "      <td>ATI</td>\n",
       "      <td>ATI</td>\n",
       "      <td></td>\n",
       "      <td>è·å–æ¨¡ç»„å‚å•†ä¿¡æ¯</td>\n",
       "      <td></td>\n",
       "      <td>[{\"name\": \"&lt;manufacturer&gt;\", \"desc\": \"æ¨¡ç»„å‚å•†ä¿¡æ¯ã€äº§å“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT+GMR</td>\n",
       "      <td>æŸ¥è¯¢ç‰ˆæœ¬ä¿¡æ¯</td>\n",
       "      <td>æ‰§è¡Œ;æŸ¥è¯¢</td>\n",
       "      <td>AT+GMR</td>\n",
       "      <td>AT+GMR</td>\n",
       "      <td></td>\n",
       "      <td>æŸ¥è¯¢ç‰ˆæœ¬ä¿¡æ¯</td>\n",
       "      <td></td>\n",
       "      <td>[{\"name\": \"&lt;reversion&gt;\", \"desc\": \"æ¨¡ç»„è½¯ä»¶ç‰ˆæœ¬ä¿¡æ¯\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT+CSQ</td>\n",
       "      <td>è·å–ä¿¡å·å¼ºåº¦</td>\n",
       "      <td>æ‰§è¡Œ;æŸ¥è¯¢</td>\n",
       "      <td>AT+CSQ</td>\n",
       "      <td>AT+CSQ</td>\n",
       "      <td></td>\n",
       "      <td>è·å–ä¿¡å·å¼ºåº¦</td>\n",
       "      <td></td>\n",
       "      <td>[{\"name\": \"&lt;signal&gt;\", \"desc\": \"ä¿¡å·å¼ºåº¦CSQ\", \"valm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       å‘½ä»¤      å‘½ä»¤æ ‡é¢˜   å‘½ä»¤ç±»å‹    å‘½ä»¤æ ¼å¼    ç¤ºä¾‹å‘½ä»¤ ç¤ºä¾‹å“åº”      åŠŸèƒ½æè¿° å¤‡æ³¨  \\\n",
       "0     ATI  è·å–æ¨¡ç»„å‚å•†ä¿¡æ¯  æ‰§è¡Œ;æŸ¥è¯¢     ATI     ATI       è·å–æ¨¡ç»„å‚å•†ä¿¡æ¯      \n",
       "1  AT+GMR    æŸ¥è¯¢ç‰ˆæœ¬ä¿¡æ¯  æ‰§è¡Œ;æŸ¥è¯¢  AT+GMR  AT+GMR         æŸ¥è¯¢ç‰ˆæœ¬ä¿¡æ¯      \n",
       "2  AT+CSQ    è·å–ä¿¡å·å¼ºåº¦  æ‰§è¡Œ;æŸ¥è¯¢  AT+CSQ  AT+CSQ         è·å–ä¿¡å·å¼ºåº¦      \n",
       "\n",
       "                                              å‚æ•°JSON  \n",
       "0  [{\"name\": \"<manufacturer>\", \"desc\": \"æ¨¡ç»„å‚å•†ä¿¡æ¯ã€äº§å“...  \n",
       "1  [{\"name\": \"<reversion>\", \"desc\": \"æ¨¡ç»„è½¯ä»¶ç‰ˆæœ¬ä¿¡æ¯\", \"...  \n",
       "2  [{\"name\": \"<signal>\", \"desc\": \"ä¿¡å·å¼ºåº¦CSQ\", \"valm...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 1 â€” Word â†’ CSVï¼ˆè§£æåµŒå¥—è¡¨æ ¼ â†’ valmapï¼‰\n",
    "import os, re, json\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.oxml.text.paragraph import CT_P\n",
    "from docx.oxml.table import CT_Tbl\n",
    "from lxml import etree\n",
    "\n",
    "IN_WORD = \"at-parameter-demo.docx\"\n",
    "CSV_DIR = \"data\"\n",
    "CSV_OUT = os.path.join(CSV_DIR, \"extracted_commands.csv\")\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "\n",
    "PARAM_HEADING_PAT = re.compile(r'^\\s*å‚æ•°(è¯´æ˜|è¡¨)?\\s*[:ï¼š]?\\s*$', re.I)\n",
    "CMD_LINE_PAT = re.compile(r'^\\s*(AT\\S*?)\\s*[:ï¼š]\\s*(.*)$')\n",
    "\n",
    "def is_param_heading(text): return bool(PARAM_HEADING_PAT.match(text or \"\"))\n",
    "def is_cmd_heading(text): return bool(CMD_LINE_PAT.match(text or \"\"))\n",
    "\n",
    "def iter_ordered_blocks(doc):\n",
    "    body = doc._element.body; tbl_idx = 0\n",
    "    for child in body.iterchildren():\n",
    "        if isinstance(child, CT_P):\n",
    "            text = ''.join([t.text for t in child.xpath('.//w:t') if t.text]).strip()\n",
    "            yield ('p', text)\n",
    "        elif isinstance(child, CT_Tbl):\n",
    "            table_obj = doc.tables[tbl_idx]; tbl_idx += 1; yield ('tbl', table_obj)\n",
    "\n",
    "def cell_plain_text(cell):\n",
    "    parts = [p.text.strip() for p in cell.paragraphs if p.text and p.text.strip()]\n",
    "    return '\\n'.join(parts).strip()\n",
    "\n",
    "def find_nested_tbls_in_cell(cell):\n",
    "    xml_str = cell._tc.xml\n",
    "    root = etree.fromstring(xml_str.encode('utf-8'))\n",
    "    ns = {\"w\": \"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"}\n",
    "    return root.findall('.//w:tbl', ns), ns\n",
    "\n",
    "def tbl_rows_as_text(tbl, ns):\n",
    "    rows = []\n",
    "    for r in tbl.findall('.//w:tr', ns):\n",
    "        cells = r.findall('.//w:tc', ns)\n",
    "        row = [''.join(tn.text for tn in c.iterfind('.//w:t', ns) if tn.text).strip() for c in cells]\n",
    "        rows.append(row)\n",
    "    return rows\n",
    "\n",
    "def looks_like_header(row):\n",
    "    hdr = ' '.join(row[:3])\n",
    "    return any(k in hdr for k in ['å‚æ•°','åç§°','Name','æè¿°','è¯´æ˜','å–å€¼','å€¼','value','meaning'])\n",
    "\n",
    "def nested_table_to_valmap(rows):\n",
    "    if not rows: return {}\n",
    "    start = 1 if looks_like_header(rows[0]) else 0\n",
    "    kv = {}\n",
    "    for r in rows[start:]:\n",
    "        if not r: continue\n",
    "        key = (r[0] or '').strip()\n",
    "        val = ' | '.join([c for c in r[1:] if c and c.strip()]) if len(r)>1 else ''\n",
    "        if key: kv[key] = val\n",
    "    return kv\n",
    "\n",
    "def cell_valmap_from_nested_table(cell):\n",
    "    tbls, ns = find_nested_tbls_in_cell(cell)\n",
    "    mapping = {}\n",
    "    for t in tbls:\n",
    "        rows = tbl_rows_as_text(t, ns)\n",
    "        mapping.update(nested_table_to_valmap(rows))\n",
    "    return mapping\n",
    "\n",
    "def parse_enum_map_fuzzy(text):\n",
    "    if not text: return {}\n",
    "    segs = re.split(r'[ï¼Œ,;ï¼›\\n]+', text.strip())\n",
    "    m = {}\n",
    "    for s in segs:\n",
    "        s = s.strip()\n",
    "        if not s: continue\n",
    "        if ':' in s or 'ï¼š' in s:\n",
    "            k,v = re.split(r'[:ï¼š]',s,1); k,v=k.strip(),v.strip()\n",
    "        else:\n",
    "            m2 = re.match(r'^(\\S+)\\s*(?:->|â†’|=>|-|â€”|\\s)\\s*(.+)$', s)\n",
    "            if m2: k,v=m2.group(1).strip(),m2.group(2).strip()\n",
    "            else:\n",
    "                m3 = re.match(r'^([A-Za-z0-9\\+\\-\\.]+)\\s+(.+)$', s)\n",
    "                if m3: k,v=m3.group(1).strip(),m3.group(2).strip()\n",
    "                else: continue\n",
    "        if k: m[k]=v\n",
    "    return m\n",
    "\n",
    "def extract_word_to_csv(docx_path, csv_out):\n",
    "    if not os.path.exists(docx_path):\n",
    "        raise FileNotFoundError(f\"æœªæ‰¾åˆ° Word æ–‡ä»¶: {docx_path}\")\n",
    "    doc = Document(docx_path)\n",
    "    seq = list(iter_ordered_blocks(doc))\n",
    "    results = []; i = 0\n",
    "    while i < len(seq):\n",
    "        typ,obj = seq[i]\n",
    "        if typ == 'p':\n",
    "            m = CMD_LINE_PAT.match(obj)\n",
    "            if m:\n",
    "                cmd, title = m.group(1), (m.group(2) or '').strip()\n",
    "                params_all = []; j = i + 1\n",
    "                while j < len(seq):\n",
    "                    t2,o2 = seq[j]\n",
    "                    if t2 == 'p' and is_cmd_heading(o2): break\n",
    "                    if t2 == 'p' and is_param_heading(o2):\n",
    "                        k = j + 1\n",
    "                        while k < len(seq) and seq[k][0] == 'tbl':\n",
    "                            table = seq[k][1]\n",
    "                            for r in table.rows:\n",
    "                                cols = r.cells\n",
    "                                if not any(c.text.strip() for c in cols): continue\n",
    "                                name = cell_plain_text(cols[0]) if len(cols)>0 else ''\n",
    "                                desc = cell_plain_text(cols[1]) if len(cols)>1 else ''\n",
    "                                valmap = {}\n",
    "                                if len(cols)>2:\n",
    "                                    valmap = cell_valmap_from_nested_table(cols[2]) or parse_enum_map_fuzzy(cell_plain_text(cols[2]))\n",
    "                                if not valmap and len(cols)>1:\n",
    "                                    valmap = cell_valmap_from_nested_table(cols[1]) or parse_enum_map_fuzzy(desc)\n",
    "                                if name in ('å‚æ•°','å‚æ•°å','Name') and any(x in desc for x in ['æè¿°','è¯´æ˜','Description']): continue\n",
    "                                params_all.append({'name':name,'desc':desc,'valmap':valmap})\n",
    "                            k += 1\n",
    "                        j = k; continue\n",
    "                    j += 1\n",
    "                if params_all:\n",
    "                    results.append({'å‘½ä»¤':cmd,'å‘½ä»¤æ ‡é¢˜':title,'å‘½ä»¤ç±»å‹':'æ‰§è¡Œ;æŸ¥è¯¢','å‘½ä»¤æ ¼å¼':cmd,\n",
    "                                    'ç¤ºä¾‹å‘½ä»¤':cmd,'ç¤ºä¾‹å“åº”':'','åŠŸèƒ½æè¿°':title,'å¤‡æ³¨':'',\n",
    "                                    'å‚æ•°JSON':json.dumps(params_all,ensure_ascii=False)})\n",
    "                i = j; continue\n",
    "        i += 1\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(csv_out,index=False,encoding='utf-8-sig')\n",
    "    print(f'âœ… æå– {len(df)} æ¡å‘½ä»¤ â†’ {csv_out}')\n",
    "    return df\n",
    "\n",
    "df_csv = extract_word_to_csv(IN_WORD, CSV_OUT)\n",
    "df_csv.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eba16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²ç”Ÿæˆ YAML â†’ data/all_commands.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2 â€” CSV â†’ YAMLï¼ˆä¿ç•™ valmapï¼‰\n",
    "import yaml, json\n",
    "YAML_OUT = os.path.join(CSV_DIR, \"all_commands.yaml\")\n",
    "\n",
    "def csv_to_yaml(csv_path, yaml_path):\n",
    "    df = pd.read_csv(csv_path, dtype=str).fillna(\"\")\n",
    "    cmds = []\n",
    "    for _,r in df.iterrows():\n",
    "        params = json.loads(r['å‚æ•°JSON']) if r['å‚æ•°JSON'] else []\n",
    "        cmds.append({\n",
    "            'command':r['å‘½ä»¤'],'title':r['å‘½ä»¤æ ‡é¢˜'],\n",
    "            'type':[t.strip() for t in r['å‘½ä»¤ç±»å‹'].split(';') if t.strip()],\n",
    "            'formats':[f.strip() for f in r['å‘½ä»¤æ ¼å¼'].split('|') if f.strip()] or [r['å‘½ä»¤æ ¼å¼']],\n",
    "            'parameters':params,\n",
    "            'examples':[{'cmd':c.strip(),'resp':e.strip()} for c,e in zip((r['ç¤ºä¾‹å‘½ä»¤'] or '').split('|'),(r['ç¤ºä¾‹å“åº”'] or '').split('|')) if c.strip() or e.strip()],\n",
    "            'description':r.get('åŠŸèƒ½æè¿°',''),'notes':r.get('å¤‡æ³¨','')\n",
    "        })\n",
    "    with open(yaml_path,'w',encoding='utf-8') as f:\n",
    "        yaml.safe_dump({'commands':cmds}, f, allow_unicode=True, sort_keys=False)\n",
    "    print(f'âœ… å·²ç”Ÿæˆ YAML â†’ {yaml_path}')\n",
    "\n",
    "csv_to_yaml(CSV_OUT, YAML_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cbca1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RST å·²ç”Ÿæˆåˆ° data/rst_output\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3 â€” YAML â†’ RSTï¼ˆä½¿ç”¨ p.valmap æ¨¡æ¿ï¼‰\n",
    "from jinja2 import Template\n",
    "import yaml, os\n",
    "\n",
    "RST_DIR = os.path.join(\"data\",\"rst_output\")\n",
    "os.makedirs(RST_DIR, exist_ok=True)\n",
    "\n",
    "TEMPLATE_STR = '''\n",
    "{{ cmd.command }}\n",
    "{{ '=' * cmd.command|length }}\n",
    "\n",
    "**Title**: {{ cmd.title }}\n",
    "**Types**: {{ cmd.type|join(', ') }}\n",
    "\n",
    "Formats::\n",
    "{%- for f in cmd.formats %}\n",
    "   {{ f }}\n",
    "{%- endfor %}\n",
    "\n",
    "Parameters\n",
    "----------\n",
    ".. list-table::\n",
    "   :header-rows: 1\n",
    "   :widths: 18 34 48\n",
    "\n",
    "   * - Name\n",
    "     - Description\n",
    "     - Values\n",
    "{%- for p in cmd.parameters %}\n",
    "   * - {{ p.name }}\n",
    "     - {{ p.desc or 'â€”' }}\n",
    "     - {%- if p.valmap %}\n",
    "       .. list-table::\n",
    "          :header-rows: 1\n",
    "          :widths: 20 40\n",
    "\n",
    "          * - Key\n",
    "            - Value\n",
    "{%- for k,v in p.valmap.items() %}\n",
    "          * - {{ k }}\n",
    "            - {{ v }}\n",
    "{%- endfor %}\n",
    "       {%- else %} N/A {%- endif %}\n",
    "{%- endfor %}\n",
    "'''\n",
    "\n",
    "RST_TMPL = Template(TEMPLATE_STR)\n",
    "\n",
    "def yaml_to_rst(yaml_path, rst_dir):\n",
    "    with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    cmds = data.get('commands', [])\n",
    "    for cmd in cmds:\n",
    "        text = RST_TMPL.render(cmd=cmd)\n",
    "        fname = f\"{cmd['command']}.rst\"\n",
    "        with open(os.path.join(rst_dir, fname), 'w', encoding='utf-8') as fo:\n",
    "            fo.write(text)\n",
    "    index_lines = ['AT Manual','=========','','.. toctree::','   :maxdepth: 1','']\n",
    "    for cmd in cmds:\n",
    "        index_lines.append(f\"   {cmd['command']}\")\n",
    "    with open(os.path.join(rst_dir,'index.rst'),'w',encoding='utf-8') as fo:\n",
    "        fo.write('\\n'.join(index_lines))\n",
    "    print(f'âœ… RST å·²ç”Ÿæˆåˆ° {rst_dir}')\n",
    "\n",
    "yaml_to_rst(YAML_OUT, RST_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2565adf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ docs/ å·²å­˜åœ¨ï¼Œè·³è¿‡ sphinx-quickstart åˆå§‹åŒ–ã€‚\n",
      "âœ… RST å·²å¤åˆ¶åˆ° docs/source/\n"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "DOCS_DIR = 'docs'\n",
    "if not os.path.exists(DOCS_DIR):\n",
    "    !sphinx-quickstart {DOCS_DIR} --sep --project 'AT Command Manual' --author 'Doc Team' --release '1.0' -q\n",
    "else:\n",
    "    print('âš ï¸ docs/ å·²å­˜åœ¨ï¼Œè·³è¿‡ sphinx-quickstart åˆå§‹åŒ–ã€‚')\n",
    "\n",
    "conf_py = os.path.join(DOCS_DIR, 'source', 'conf.py')\n",
    "if os.path.exists(conf_py):\n",
    "    with open(conf_py, 'a', encoding='utf-8') as f:\n",
    "        f.write('\\nhtml_theme = \"sphinx_rtd_theme\"\\n')\n",
    "\n",
    "shutil.copytree('data/rst_output', os.path.join(DOCS_DIR, 'source'), dirs_exist_ok=True)\n",
    "print('âœ… RST å·²å¤åˆ¶åˆ° docs/source/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525cbd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01mRunning Sphinx v8.2.3\u001b[39;49;00m\n",
      "\u001b[01mloading translations [en]... \u001b[39;49;00mdone\n",
      "\u001b[01mloading pickled environment... \u001b[39;49;00mThe configuration has changed (2 options: 'html_permalinks_icon', 'jquery_use_sri')\n",
      "done\n",
      "\u001b[01mbuilding [mo]: \u001b[39;49;00mtargets for 0 po files that are out of date\n",
      "\u001b[01mwriting output... \u001b[39;49;00m\n",
      "\u001b[01mbuilding [html]: \u001b[39;49;00mtargets for 4 source files that are out of date\n",
      "\u001b[01mupdating environment: \u001b[39;49;00m0 added, 4 changed, 0 removed\n",
      "\u001b[2K\u001b[01mreading sources... \u001b[39;49;00m[100%] \u001b[35mindex\u001b[39;49;00mm\n",
      "\u001b[01mlooking for now-outdated files... \u001b[39;49;00mnone found\n",
      "\u001b[01mpickling environment... \u001b[39;49;00mdone\n",
      "\u001b[01mchecking consistency... \u001b[39;49;00mdone\n",
      "\u001b[01mpreparing documents... \u001b[39;49;00mdone\n",
      "\u001b[01mcopying assets... \u001b[39;49;00m\n",
      "\u001b[01mcopying static files... \u001b[39;49;00m\n",
      "Writing evaluated template result to /Users/pika/Documents/GitHub/docs-as-code-learning/pipeline-1008/docs/build/html/_static/basic.css\n",
      "Writing evaluated template result to /Users/pika/Documents/GitHub/docs-as-code-learning/pipeline-1008/docs/build/html/_static/language_data.js\n",
      "Writing evaluated template result to /Users/pika/Documents/GitHub/docs-as-code-learning/pipeline-1008/docs/build/html/_static/documentation_options.js\n",
      "Writing evaluated template result to /Users/pika/Documents/GitHub/docs-as-code-learning/pipeline-1008/docs/build/html/_static/js/versions.js\n",
      "\u001b[01mcopying static files: \u001b[39;49;00mdone\n",
      "\u001b[01mcopying extra files... \u001b[39;49;00m\n",
      "\u001b[01mcopying extra files: \u001b[39;49;00mdone\n",
      "\u001b[01mcopying assets: \u001b[39;49;00mdone\n",
      "\u001b[2K\u001b[01mwriting output... \u001b[39;49;00m[100%] \u001b[32mindex\u001b[39;49;00mm\u001b[01mwriting output... \u001b[39;49;00m[ 50%] \u001b[32mAT+GMR\u001b[39;49;00m\n",
      "\u001b[01mgenerating indices... \u001b[39;49;00mgenindex done\n",
      "\u001b[01mwriting additional pages... \u001b[39;49;00msearch done\n",
      "\u001b[01mdumping search index in English (code: en)... \u001b[39;49;00mdone\n",
      "\u001b[01mdumping object inventory... \u001b[39;49;00mdone\n",
      "\u001b[01mbuild succeeded.\u001b[39;49;00m\n",
      "\n",
      "The HTML pages are in build/html.\n",
      "\n",
      "âœ… æ„å»ºå®Œæˆï¼Œæ‰“å¼€ï¼šdocs/build/html/index.html\n"
     ]
    }
   ],
   "source": [
    "!make -C docs html\n",
    "print('\\nâœ… æ„å»ºå®Œæˆï¼Œæ‰“å¼€ï¼šdocs/build/html/index.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
